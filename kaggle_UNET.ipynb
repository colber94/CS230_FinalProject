{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/tdrobbins/unet.git\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import unet\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import sen2lte_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session settings \n",
    "TAG   = \"ca_1280-resize_256crop\" # descriptive name for the run\n",
    "SEED  = 1                       # random seed used by data generators\n",
    "\n",
    "# Preprocessing parameters\n",
    "RESIZE      = 1280      # downsample raw satellite image to RESIZExRESIZE\n",
    "CROP        = 256       # split downsampled image into CROPxCROP images\n",
    "N_CLASSES   = 5         # [2, 5, or 10] how many access classes to train\n",
    "STATES      = [\"ca\"]    # which states' data to use (ca,co,or available)\n",
    "\n",
    "TRAIN_SPLIT   = 0.8     # proportion of data to use for training\n",
    "CLASS_WEIGHTS = \"auto\"  # set class weights automatically (\"auto\") or with a list\n",
    "NORMALIZE     = False   # whether the data generator should normalize the data\n",
    "\n",
    "# Model parameters\n",
    "CONTINUE      = False   # [None, /path/model] previous model to contiune fitting (overrides model settings)\n",
    "LEARNING_RATE = 0.0001 \n",
    "LAYER_DEPTH   = 5       \n",
    "ROOT_FILTERS  = 64\n",
    "DROPOUT       = 0.0\n",
    "\n",
    "# Training parameters \n",
    "EPOCHS        = 250\n",
    "BATCH_SIZE    = 16\n",
    "TRANSFER      = False\n",
    "\n",
    "# Output path settings\n",
    "SAVE_ROOT = \"/kaggle/working/logs/\"\n",
    "SAVE_PATH = SAVE_ROOT + TAG + \"/\"\n",
    "CP_PATH = SAVE_PATH+\"checkpoint/\"\n",
    "MODEL_PATH = SAVE_PATH+\"model/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# loading input images and masks for each state\n",
    "X = []\n",
    "Y = []\n",
    "for s in STATES:\n",
    "    sat_files = np.sort(glob(\"/kaggle/input/sen2lte/Data/sentinel2/{}/composite/*.tif\".format(s)))\n",
    "    cell_files = np.sort(glob(\"/kaggle/input/sen2lte/Data/celltowers/sentinel2/{}/*.jpg\".format(s)))\n",
    "    lte_files = np.sort(glob(\"/kaggle/input/sen2lte/Data/fcc477actual/sentinel2/{}/cat{}*.jpg\".format(s,N_CLASSES)))\n",
    "    \n",
    "    X.append(utils.load_sat_imgs(sat_files, cell_files,resize=RESIZE, crop_size=CROP))\n",
    "    Y.append(utils.load_masks(lte_files, N_CLASSES, crop_size=CROP, resize=RESIZE, onehot=True))\n",
    "\n",
    "X = np.concatenate(X)\n",
    "Y = np.concatenate(Y)\n",
    "m,nx,ny,nchannels = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and dev sets   \n",
    "(X_train, Y_train), (X_dev, Y_dev) = utils.split_train_dev(X,Y,split=TRAIN_SPLIT, batch_size=BATCH_SIZE, seed=SEED)\n",
    "\n",
    "# Experimenting with settings CLASS_WEIGHTS automatically\n",
    "if CLASS_WEIGHTS == \"auto\":\n",
    "    counts = np.unique(Y_train.argmax(-1),return_counts=True)[1]\n",
    "    CLASS_WEIGHTS = np.round(max(counts)/counts).astype(np.uint)\n",
    "\n",
    "if CLASS_WEIGHTS is not None:\n",
    "    Y_train = Y_train*CLASS_WEIGHTS\n",
    "    Y_dev = Y_dev*CLASS_WEIGHTS\n",
    "\n",
    "# Create generators using split datasets\n",
    "train_datagen = ImageDataGenerator(featurewise_center=NORMALIZE,featurewise_std_normalization=NORMALIZE)\n",
    "dev_datagen = ImageDataGenerator(featurewise_center=NORMALIZE,featurewise_std_normalization=NORMALIZE)\n",
    "\n",
    "if NORMALIZE:\n",
    "    train_datagen.fit(X_train)\n",
    "    dev_datagen.fit(X_train)\n",
    "\n",
    "train_set = train_datagen.flow(X_train,Y_train,batch_size=BATCH_SIZE)\n",
    "dev_set = dev_datagen.flow(X_dev,Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONTINUE: \n",
    "    unet_model = tf.keras.models.load_model(CONTINUE)\n",
    "\n",
    "else:\n",
    "  # Building the model\n",
    "  unet_model = unet.build_model(nx, ny,\n",
    "                            channels = nchannels,\n",
    "                            num_classes = N_CLASSES,\n",
    "                            layer_depth = LAYER_DEPTH,\n",
    "                            filters_root= ROOT_FILTERS,\n",
    "                            dropout_rate = DROPOUT,\n",
    "                            padding = \"same\"\n",
    "                            )\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=SAVE_PATH, histogram_freq=1)\n",
    "best_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=CP_PATH,save_weights_only=False,save_best_only=True,monitor=\"val_Recall\",mode=\"max\",verbose=True)\n",
    "epoch_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=MODEL_PATH,save_weights_only=False,period=50,verbose=True)\n",
    "callbacks = [tensorboard_callback, best_checkpoint, epoch_checkpoint]\n",
    "\n",
    "unet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[\"Precision\", \"Recall\",\"CategoricalAccuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRANSFER: unet_model.load_weights(TRANSFER)\n",
    "\n",
    "model_history = unet_model.fit(train_set,\n",
    "                              validation_data=dev_set,\n",
    "                              epochs=EPOCHS, \n",
    "                              steps_per_epoch=len(X_train)//BATCH_SIZE,\n",
    "                              callbacks=callbacks)\n",
    "\n",
    "unet_model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_metrics(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_cropped_examples(unet_model, (X_train,Y_train), (X_dev,Y_dev), RESIZE, CROP, N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = np.random.choice(range(len(sat_files)),3)\n",
    "utils.plot_uncropped_examples(unet_model, sat_files[select], cell_files[select], lte_files[select], RESIZE, CROP, N_CLASSES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
