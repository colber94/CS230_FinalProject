{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E8B6ES4v95gk"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/tdrobbins/unet.git@nchannels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dqlHpp5i3CgF"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "import tensorflow as tf\n",
    "import unet\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pp\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e4_XrGAfAsh5"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzlN5zwYaTKB"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBCHx4Ox8irk"
   },
   "outputs": [],
   "source": [
    "# Loading and preprocessing data\n",
    "\n",
    "# Preprocessing paramters\n",
    "CROP        = 1024    # split raw images into many CROPxCROP images\n",
    "RESIZE      = 256    # downsample cropped image to RESIZExRESIZE\n",
    "N_CLASSES   = 4      # 2, 4, or 10\n",
    "CELLTOWERS  = True  # currently doesn't work... may require forking unet due to dumb bug\n",
    "\n",
    "# Loading satellite images into an array\n",
    "sat_files = np.sort(glob(\"/content/drive/My Drive/cs230/sen2_composite/*.tif\"))\n",
    "X = utils.load_data(sat_files,crop_size=CROP,resize=RESIZE,scale=1./256.)\n",
    "\n",
    "# Loading nclasses version of lte mask images into an array\n",
    "lte_files = np.sort(glob(\"/content/drive/My Drive/cs230/lte/cat{}*.tif\".format(N_CLASSES)))\n",
    "Y = utils.load_data(lte_files,crop_size=CROP,resize=RESIZE)\n",
    "\n",
    "# Resizing and scaling images\n",
    "\n",
    "if CELLTOWERS:\n",
    "  cell_files = np.sort(glob(\"/content/drive/My Drive/cs230/celltowers/*.tif\"))\n",
    "  cell_arr = utils.load_data(cell_files,crop_size=CROP,resize=RESIZE)\n",
    "  X = np.asarray([np.dstack((x,c)) for x,c in zip(X,cell_arr)])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UZSVBY4A-Cq"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BYQbd8UdnRsM"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir /content/unet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kuuwPTTGVjyQ"
   },
   "outputs": [],
   "source": [
    " # Setting up the datasets for tf\n",
    "TRAIN_SPLIT   = 0.8\n",
    "BATCH_SIZE    = 8\n",
    "CLASS_WEIGHTS = [5,5,5,1]\n",
    "\n",
    "split = np.round(X.shape[0]*TRAIN_SPLIT)\n",
    "\n",
    "sat_lte_dataset = tf.data.Dataset.from_tensor_slices((X,tf.one_hot(Y,N_CLASSES)*CLASS_WEIGHTS)).shuffle(1000)\n",
    "train_set = sat_lte_dataset.take(split).batch(BATCH_SIZE)\n",
    "test_set = sat_lte_dataset.skip(split).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hjNOWqO-4aKh"
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "LEARNING_RATE = 0.0001\n",
    "LAYER_DEPTH   = 5\n",
    "ROOT_FILTERS  = 64\n",
    "DROPOUT       = 0.0\n",
    "\n",
    "# Building the model\n",
    "unet_model = unet.build_model(X.shape[1], X.shape[2],\n",
    "                          channels = X.shape[3],\n",
    "                          num_classes = N_CLASSES,\n",
    "                          layer_depth = LAYER_DEPTH,\n",
    "                          filters_root= ROOT_FILTERS,\n",
    "                          dropout_rate = DROPOUT,\n",
    "                          padding = \"same\"\n",
    "                          )\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"/content/unet/\"+datetime.datetime.now().strftime(\"%y%m%d-%H%M\"), histogram_freq=1)\n",
    "\n",
    "unet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xb5p-zV6EukX"
   },
   "outputs": [],
   "source": [
    "# Dataset parameters \n",
    "EPOCHS        = 5\n",
    "model_history = unet_model.fit(train_set, \n",
    "                               validation_data=test_set,\n",
    "                               epochs=EPOCHS, \n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               callbacks=[tensorboard_callback])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "UNET_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (cs230)",
   "language": "python",
   "name": "cs230"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
